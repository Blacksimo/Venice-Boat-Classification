# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h3VEVNvRwRxydsT7OvtXqbqWf7Qy4jE_
"""

from google.colab import drive
drive.mount('/content/drive/')

cd "drive/My Drive"

from tqdm import tqdm
from textwrap import wrap
from sklearn.utils import shuffle
from sklearn import preprocessing

from skimage.util import random_noise
from skimage.color import rgb2gray
from skimage import util
from skimage.transform import rotate, resize
from skimage import exposure
from scipy import ndimage

import os, os.path
import random
import matplotlib.pyplot as plt
import numpy as np
import pickle
import cv2
from random import randint

trainFolder = 'train/'
testFolder = 'test/'
IMAGE_SIZE = 90

def randomNoise(originalImage):
  return random_noise(originalImage)
def colorToGray(originalImage):
  return rgb2gray(originalImage)
def colorInversion(originalImage):
  return util.invert(originalImage)
def rotated(originalImage):
  angle = [45, -45, 135, -135]
  return rotate(originalImage, random.choice(angle))
def changeIntensity(originalImage):
  v_min, v_max = np.percentile(originalImage, (0.2, 99.8))
  return exposure.rescale_intensity(originalImage, in_range=(v_min, v_max))
def gammaCorrection(originalImage):
  return exposure.adjust_gamma(originalImage, gamma=0.4, gain=0.9)
def blur(originalImage):
  return ndimage.uniform_filter(originalImage, size=11)
def changeSize(originalImage):
  return resize(originalImage, (IMAGE_SIZE, IMAGE_SIZE))

adjustments = [randomNoise, colorInversion, rotated, changeIntensity, gammaCorrection, blur]

# Save data to file
def save_data(X, y, name):
    with open(name + '.p', 'wb') as f:
        pickle.dump({'features':X, 'labels':y}, f, pickle.HIGHEST_PROTOCOL)

# Load data from file
def load_data(name):
    with open(name + '.p', 'rb') as f:
        return pickle.load(f)

categoryList = list()
for category in os.listdir(trainFolder):
  if category not in categoryList and len(os.listdir(trainFolder+category)) > 100:
    categoryList.append(category)
categoryList.sort()
'''print("Number of Categories: ", len(categoryList))
print("\t\n".join(categoryList))'''
testCategoryList = list()
with open(testFolder + "ground_truth.txt") as file:
  for row in file:
    info = row.split(";")
    info = info[1].replace("\n", "").replace(":", "").replace(" ", "")
    if info not in testCategoryList:
      testCategoryList.append(info)
testCategoryList.sort()
'''print("Number of Categories: ", len(testCategoryList))
print("\t\n".join(testCategoryList))'''
matchingCategories = list()
for el in categoryList:
  if el in testCategoryList and el not in matchingCategories:
    matchingCategories.append(el)
matchingCategories.sort()
print("\t\n".join(matchingCategories))

X_train = list()
y_train = list()
if os.path.isfile('trainingResize.p'):
    print('Training data file found!')
    with open('trainingResize.p', mode='rb') as f:
      train = pickle.load(f)
    X_train, y_train = train['features'], train['labels']
else:
  
  for category in os.listdir(trainFolder):
    if category in matchingCategories:
      files = os.listdir(trainFolder + category)
      # Gap the image numers to 300
      gap = 300
      if len(files)<gap:
        gap = len(files)

      # If file is in classes to be classified
      i = matchingCategories.index(category)
      pbar = tqdm(range(gap), position=i)
      pbar.set_description('Processing \'' + category + '\'')

      # For each file
      for img in pbar:
          f = files[img]
          image = plt.imread(trainFolder + category + "/" + f)
          image = changeSize(image)
          #image = colorToGray(image)
          X_train.append(image)
          y_train.append(matchingCategories.index(category))

  X_train = np.array(X_train)
  y_train = np.array(y_train)

  # Save dictionary
  save_data(X_train, y_train, 'trainingResize')
  print('\nTraining saved to file!')

print(X_train.shape)

X_test = list()
y_test = list()
if os.path.isfile('testResize.p'):
    print('Test data file found!')
    with open('testResize.p', mode='rb') as f:
      test = pickle.load(f)
    X_test, y_test = test['features'], test['labels']
else:  
  testDictionary = {}
  with open(testFolder + 'ground_truth.txt', 'r') as file:
    for row in file:
      image, categoryName = row.strip().split(';')
      categoryName = categoryName.replace(":", "").replace(" ", "")
      #print(categoryName)
      if categoryName in matchingCategories:
          testDictionary[image] = categoryName
  #print(testDictionary)
  pbar = tqdm(range(len(testDictionary)), position = 0)
  pbar.set_description('Processing test data')

  keys = list(testDictionary)

  for f in pbar:
      image = plt.imread(testFolder + "/" + keys[f])
      #image = colorToGray(image)
      image = changeSize(image)
      X_test.append(image)
      y_test.append(matchingCategories.index(testDictionary[keys[f]]))

  X_test = np.array(X_test)
  y_test = np.array(y_test)

  # Save dictionary
  save_data(X_test, y_test, 'testResize')
  print('\nTest saved to file!')

print(X_test.shape)

labelCount = {}
total = 0
for category in os.listdir(trainFolder):
  if category in matchingCategories:
    labelCount[category] = len(os.listdir(trainFolder + category))

nMax = 0
for k in labelCount:
  print('{:>3}'.format(labelCount[k]),'{:>3}'.format(k))
  nMax = max(nMax, labelCount[k])

def showImages(before, after, op):
    fig, axes = plt.subplots(nrows=1, ncols=2)
    ax = axes.ravel()
    ax[0].imshow(before, cmap='gray')
    ax[0].set_title("Original image")

    ax[1].imshow(after, cmap='gray')
    ax[1].set_title(op + " image")
    if op == "Rescaled":
        ax[0].set_xlim(0, 500)
        ax[0].set_ylim(500, 0)
    else:        
        ax[0].axis('off')
        ax[1].axis('off')
    plt.tight_layout()

showImages(X_train[10], X_test[10],"proa")

if os.path.isfile('training_augmentedResize.p'):
  print('Training file found!')
  with open('training_augmentedResize.p', mode='rb') as f:
    trainAugmented = pickle.load(f)
  X_train, y_train = trainAugmented['features'], trainAugmented['labels']
else:
  print('Training augmented file not found\n')
  count = 0
  #randomTaken = list()
  for label in labelCount:
    nActual = labelCount[label]
    fileList = os.listdir(trainFolder+label)
    if nActual<300:
      pbar = tqdm(range(300-nActual), position = count)
      pbar.set_description('Processing images')
      for f in pbar:
        randomPick = random.choice(fileList)
        #print(label)
        #oldImage = plt.imread(trainFolder + label + "/" + randomPick)
        #newImage = colorToGray(plt.imread(trainFolder + label + "/" + randomPick))
        newImage = changeSize(plt.imread(trainFolder + label + "/" + randomPick))
        newImage = random.choice(adjustments)(newImage)
        #newImage = np.append([], newImage)
        #print(X_train.shape)
        X_train = np.append(X_train, [newImage], axis=0)
        #print(X_train.shape)
        y_train = np.append(y_train, matchingCategories.index(label))
       
        #print(len(y_trainAugmented))
      print(X_train.shape)
      count += 1
  # Save augmented data
  #X_trainAugmented = np.array(X_trainAugmented)
  #y_trainAugmented = np.array(y_trainAugmented)
  save_data(X_train, y_train, 'training_augmentedResize')
  print('Training augmented saved to file!')

print(X_train.shape)
print(y_train.shape)

# One Hot encode the labels to the variable y_one_hot
label_binarizer = preprocessing.LabelBinarizer()
label_binarizer.fit(y_train)

# Shuffle the data
X_train, y_train = shuffle(X_train, y_train)

# Encode labels
y_train = label_binarizer.transform(y_train)
y_test = label_binarizer.transform(y_test)

# Change to float32, so it can be multiplied
# against the features in TensorFlow, which are float32
X_train = X_train.astype(np.float32)
y_train = y_train.astype(np.float32)
X_test = X_test.astype(np.float32)
y_test = y_test.astype(np.float32)

height, width, depth = X_train[0].shape
print(height, width, depth)
print(y_train.shape)

import tensorflow as tf
import tflearn 
from tflearn.layers.conv import conv_2d, max_pool_2d 
from tflearn.layers.core import input_data, dropout, fully_connected 
from tflearn.layers.estimator import regression 

tf.reset_default_graph() 
convnet = input_data(shape =[None, height, width, depth], name ='input') 
  
convnet = conv_2d(convnet, 32, 5, activation ='relu') 
convnet = max_pool_2d(convnet, 5) 
  
convnet = conv_2d(convnet, 64, 5, activation ='relu') 
convnet = max_pool_2d(convnet, 5) 
  
convnet = conv_2d(convnet, 128, 5, activation ='relu') 
convnet = max_pool_2d(convnet, 5) 

convnet = fully_connected(convnet, 1024, activation ='relu') 
convnet = dropout(convnet, 0.8) 
  
convnet = fully_connected(convnet, 8, activation ='softmax') 
convnet = regression(convnet, optimizer ='adam', learning_rate = 1e-3, 
      loss ='categorical_crossentropy', name ='targets') 
  
model = tflearn.DNN(convnet, tensorboard_dir ='log')

MODEL_NAME = 'veniceBoats.model'
model.fit({'input': X_train}, {'targets': y_train}, n_epoch = 100,  
    validation_set =({'input': X_test}, {'targets': y_test}),  
    snapshot_step = 500, show_metric = True, run_id = MODEL_NAME) 
model.save(MODEL_NAME)

!pip install tensorboardcolab

from tensorboardcolab import *

tbc=TensorBoardColab()

